---
title: "Computing for the Social Sciences: Lecture 15"
subtitle: "Topics: Web-Scraping 1"
author: "Sabrina Nardin, Fall 2025"
format:
  revealjs:
    theme: simple
    slide-number: true
    incremental: false
    css: ../style/styles.css
    code-overflow: wrap
    highlight-style: github
    ratio: 16:9
    chalkboard: true
    toc: false
    center: true
---


# Agenda {.center}

::: {.agenda-list}
1. Web Scraping Introduction 
2. Our Goal: Scrape U.S. Presidential Statements
3. Two Key Concepts:   
  3.1 How the Web Works  
  3.2 What's Behind a Website  
4. The rvest Package 
:::

<span style="font-size: 0.8em; color: #666;">
*Slides last updated on **November 17, 2025**. Slides authored by Sabrina Nardin. AI used to polish slides style and fix typos.*
</span>

# 1. Web Scraping Introduction {.slide .center .middle}

---

## What is Web Scraping?

::: {.callout}
**Web Scraping: using code to automatically collect information from websites.**
:::

- If you‚Äôve ever copied and pasted data from a webpage, you‚Äôve done the same task as a scraper, just **manually and on a tiny scale**. Web scraping **automates and scales up**, so code does the collecting instead of you

- With a few R functions from **rvest** and some **HTML knowledge**, you can grab text, tables, links, and other content from a webpage at scale

---

## Web Scraping Examples

- **Company info** (names, emails, phone numbers)
- **Product details** (prices, descriptions)  
- **Real estate listings** (listings, rentals info)  
- **Government data** (press releases, bills, reports)  
- **News articles**  
- **Online archives** (historical docs, speeches)
- **Wikipedia pages**  
- **Forums** (Reddit, StackOverflow)  
- **Online reviews** (Yelp, Google)  
- **Marketplaces** (Craigslist, FB Marketplace)  
- **Social media** but harder today!  

---

## Why We are Learning Web Scraping in this Course

1. **Short applied module** to show what R can do beyond charts and data wrangling.

2. **Practical data collection skill you can use in projects**. A lot of social science data is on the web, not in ready-made CSVs; scraping is especially great for text data.

---

## Two Ways to Scrape Data from the Web

::: columns

::: column
### Directly Scraping a Website
Request data from a website directly,  
**by reading its HTML** and extract the information you need (text, tables, links, etc.)
:::

::: column
### Using an API
Request data from a website indirectly,  
**by interacting with a dedicated interface** that the website provides.
:::

:::

---

## Directly Scraping a Website

::: {.callout}
**What this means:** You read the HTML code behind a website and extract the pieces of information you need.
:::

::: {.callout}
**How it works:**  
- Websites are built using HTML, CSS, and sometimes JavaScript.  
- R via `rvest` can download a webpage, and let you extract elements like titles, tables, links, paragraphs, etc.  
- You can scrape almost anything that is publicly available.
:::

::: {.callout}
**Good beginner projects:**  
- *Wikipedia pages*  
- *Government websites* (press releases, data tables, reports)  
- *News articles*  
:::

---

## Using a Web API (Application Programming Interface)

::: {.callout}
**What this means:** You request structured data from a website through a dedicated interface.
:::

::: {.callout}
**How it works:**  
- The website provides a URL-based system where you ask for data.  
- You receive a clean, structured format (usually JSON).  
- You must follow the API‚Äôs rules: parameters, keys, rate limits, etc.
:::

::: {.callout}
**Good beginner projects:**  
- *OMDb API* (Open Movie Database) ‚Äî used in today's reading  
- *NYTimes API* (articles, comments)  
- *Census Bureau API* (with tidycensus R wrapper package)
:::

---

## Direct Scraping vs. Using an API: Pros and Cons

### **Directly Scraping the Website**

**Pros**  

- Works even when no API is available  
- Very flexible: you can extract almost anything shown on the page
- Great for text, tables, lists, articles, and public documents  

**Cons**  

- HTML structure of webpages is often messy and inconsistent  
- HTML structure can change, breaking your code  
- JavaScript-heavy pages require extra tools  

---

## Direct Scraping vs. Using an API : Pros and Cons

### **Using a Web API**

**Pros**

- Data is clean, structured, predictable (usually JSON)  
- Documentation tells you exactly what you can request  
- More stable and less fragile than scraping HTML  

**Cons**

- Only works if a website provides an API  
- You must follow the API‚Äôs rules (rate limits, keys, parameters)  
- API may not include everything shown on the webpage  

::: {.callout-tip}
If an API exists, ideally with an R wrapper, use it instead of scraping HTML.
:::


# 2. Our Goal: Scrape U.S. Presidential Statements {.slide .center .middle}

---

## Our Goal for Today

**I mentioned that we‚Äôll need a bit of theory for web scraping‚Äîhow the web works, how websites are built, some HTML, and the main rvest functions‚Äîbut before we get into that, let‚Äôs be clear about what we want to achive‚Ä¶**

::: {.callout-important}
# Goal for Today

Scrape a U.S. Presidential statement from the American Presidency Project. Specifically: speaker name,  date, title, full text of the statement 

<https://www.presidency.ucsb.edu/documents/special-message-the-congress-relative-space-science-and-exploration>
:::

---

## Scrape U.S. Presidential Statements

```r
library(tidyverse)
library(lubridate)
library(rvest)

scrape_doc <- function(url) {
  # Scrapes data from presidential pages pausing between requests
  # Args:
    # url (string): one presidential page 
  # Returns:
    # tibble: a tibble with the date, speaker, title, full text from input url

  # get HTML page
  Sys.sleep(2)
  url_contents <- read_html(x = url)

  # extract elements
  date <- html_elements(x = url_contents, css = ".date-display-single") %>%
    html_text2() %>% mdy()

  name <- html_elements(x = url_contents, css = ".diet-title a") %>%
    html_text2()
  
  title <- html_elements(x = url_contents, css = "h1") %>%
    html_text2()
  
  text <- html_elements(x = url_contents, css = "div.field-docs-content") %>%
    html_text2()
  
  # store in a data frame and return it
  url_data <- tibble(
    date = date,
    name = name,
    title = title,
    text = text
  )
  
  return(url_data)
}
```


# 3. Two Key Concepts {.slide .center .middle}

3.1 How the Web Works

3.2 What‚Äôs Behind a Website

---

## 3.1 How the Web Works

Computers communicate on the web by sending and receiving **data requests** (GET) and **data responses** (POST). Every computer has an address that other computers can refer to.

When you click on a webpage, your **web browser** (e.g., Chrome, Safari) sends a data request to the **web server** of that page (a database where all the info about that page are stored) and receives a response.

```{r fig.align = "center", echo = FALSE, out.width = "50%"}
knitr::include_graphics(path = "request_response.png", error = FALSE)
```

Image Source [at this link](https://www.linkedin.com/pulse/what-happens-when-you-enter-url-browser-he-asked-victor-ohachor)

---

## 3.1 How the Web Works

Navigating the web basically means **sending requests to different servers and receiving back various files**, mainly written in HTML and other languages.

For example, if you type `https://macss.uchicago.edu/current-student-resources` into your web browser and hit enter, these steps occur behind the scenes:

1. your web browser (Chrome, Safari, etc.) translates what you typed into a HTTP request. That request tells the web server that stores all `macss` info that you want to access the specific piece of info stored at `/current-student-resources`

1. the web server that hosts `macss` receives your request and sends back to your web browser a response code (200 for yes, etc.) and the response content (files written in HTML)

1. your browser transforms the response content into a nice visual display that might include texts, graphics, hyperlinks, etc.

**When we scrape data, we use R packages that perform these steps for us (via APIs or without)!**

---

## 3.2 What‚Äôs Behind a Website

A website is made of the following elements:

+ **HTML** *HyperText Markup Language*, is the core element of a website. HTML uses tags to organize the webpage (i.e., creates paragraphs, inserts hyperlinks, etc.), but when the page is displayed the markup language is hidden

+ **CSS** *Cascading Style Sheets*: improves the look of the page

+ **Javascript**: adds interactive elements to the page (you need "dynamic web scraping" techniques to scrape JS code, not covered in this course)

+ **Other stuff** such as images, hyperlinks, videos or multimedia

---

## 3.2 What‚Äôs Behind a Website

::: {.callout-important}
When scraping a website directly: knowing how read the HTML (and CSS) language, is fundamental  

When using an API and/or an API wrapper: less important, but still useful. 

:::


---

## HTML: HyperText Markup Language

* Important for web scraping: makes the "skeleton" or structure of a website
* Made of tags 
* Looks messy, but follows a hierarchical tree-like structure of tags within tags 

Standard HTML syntax:
```
   <html>
     <head>
        <title>general info about the page</title>
     </head>
     <body>
       <p>a paragraph with some text about the page</p>
       <p>another paragraph with more text</p>
       <p>ciao</p>
     </body>
   </html>
   
```

---

## HTML Tags

1. HTML tags are organized in a **[tree-like structure](https://www.researchgate.net/figure/HTML-source-code-represented-as-tree-structure_fig10_266611108)** and are nested

2. HTML tags **go in pairs** one on each end of the content that they display. For example: `<p>ciao</p>` only the word "ciao" shows up on the webpage, the `/` signals the end of the tag

3. HTML tags can have **attributes** which provide additional info about the tag. For example: `<p>ciao id='first'</p>`

   * `<p>` is the tag
   * `<id>` is the attribute
   * **two key attributes to remember for scraping:** `id` and `class`; they are used by CSS elements to control the visual appearance of the page
    
<!-- add more info here
https://plsc-31101.github.io/course/collecting-data-from-the-web.html#webscraping
* more frequently used tags
* more on tags attributes
* CSS 
* CSS and HTML
* see staff from my Python course, lecture 2 (e.g. every page is different, no perfect structure,etc.)
-->

---

### Most Common HTML Tags

* `html`
    * `head`
        * `title`
        * `a href`
        * `script`
    * `body`
        * `div`
            * `p`
                * `b`
            * `span`
        * `table`
            * `tr`
                * `td`
                * `td`
        * `img`

---

### Most Common HTML Tags


* `html`
    * `head`
        * `title` - title
        * `a href` - links
        * `script` - code
    * `body`
        * `div` - generic container for content (block)
            * `p` - paragraph
                * `b` - bold formatting
            * `span` - generic container for content (in line)
        * `table`
            * `tr` - row of cells
                * `td` - actual cell element 
        * `img` - image


---

## HTML Tags: Example

```html
# example of html code (this is not R code, won't run in R)
<html>
  <head>
    <title>Title</title>
    <a href="http://github.com">GitHub</a>
    <script src="https://c.js"></script>
  </head>
  <body>
    <div>
      <p>Click <b>here</b> now.</p>
      <span>Frozen</span>
    </div>
    <table style="width:100%">
      <tr>
        <td>Kristen</td>
        <td>Bell</td>
      </tr>
    </table>
  <img src="http://ia.media-imdb.com/images.png"/>
  </body>
</html>
```

**üíª Practice: Find the text content `Frozen` and the GitHub link and the text content `GitHub`**

---

## HTML Tags: Example

**Find the text content `Frozen`**

```html
<span>Frozen</span>
```

* `<span></span>` - tag name
* `Frozen` - content embedded in the tag as text

<!--In this ex. there is only one span tag: what if the HTML has multiple span tags?-->

---

## HTML Tags: Example

**Find the GitHub link and text `GitHub` embedded within it**

```html
<a href="http://github.com">GitHub</a>
```

* `<a></a>` - tag name
* `href` - tag attribute (argument)
* `"http://github.com"` - tag attribute (value)
* `GitHub` - content as text

---

## CSS: Cascading Style Sheet

Often a website has HTML tags with attributes + CSS elements. This is an example of CSS code:

```css
span {
  color: #ffffff; }
```

**Most websites use HTML tags with `class` and `id` attributes to provide ‚Äúhooks‚Äù for CSS** so the CSS "knows" where to apply CSS stylistic elements. In the example above, the `span` HTML tag can be styled using CSS `color`

See <https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/class>

<!-- https://plsc-31101.github.io/course/collecting-data-from-the-web.html#webscraping 
[CSS diner](http://flukeout.github.io) 

### HTML + CSS: example 

.pull-left[

```html
<body>
    <table id="content">
        <tr class='name'>
            <td class='firstname'>
                Kurtis
            </td>
            <td class='lastname'>
                McCoy
            </td>
        </tr>
        <tr class='name'>
            <td class='firstname'>
                Leah
            </td>
            <td class='lastname'>
                Guerrero
            </td>
        </tr>
    </table>
</body>
```

]

.pull-right[

Find the elements to use to extract:
1. The entire table
1. The general element(s) containing first names
1. Just the specific element "Kurtis"

]

-->

---

## Example

Explore the HTML structure of the website we want to scrape today:

<https://www.presidency.ucsb.edu/documents/special-message-the-congress-relative-space-science-and-exploration>


# 4. The rvest Package {.slide .center .middle}

---

## Using rvest

Rvest documentation: <https://rvest.tidyverse.org/>

- Read and save in an object the HTML source code of a webpage

- Find the specific HTLM and CSS elements from a webpage (using HTML tags or attributes, and using CSS selectors)

---

## Main functions in rvest

+ **`read_html()`**
+ **`html_elements()`**
+ **`html_element()`**
+ **`html_text2()`**
+ **`html_attr()`** 
+ **`html_table()`** 

### üíª Practice

- Open the documentation <https://rvest.tidyverse.org/index.html#usage> and explore what each function does.
- Run the example code in your own R session, try modifying it, and note what changes.
- Bring at least one question to discuss after experimenting.

---

## Main functions in rvest

The following slides and code are from the documentation: <https://rvest.tidyverse.org/index.html#usage> 

**`read_html()` reads an html page into R:**

```r
library(rvest)
starwars <- read_html("https://rvest.tidyverse.org/articles/starwars.html")
starwars |> as.character() |> cat()

```
---

## Main functions in rvest

**`html_elements()` finds elements that match a CSS selector or XPath expression.** 

Use when you expect multiple matches. Here each section corresponds to a different film:

```r
films <- starwars |> html_elements("section")
films
```

---

## Main functions in rvest

**`html_element()` finds one specific element per film section.**

Here we are saying: for each film section found before, extract one (the first, which is all we have here) `<h2>`

```r
title <- films |> 
  html_element("h2") |> 
  html_text2()
title
```

Try running `starwars |> html_element("section")` and `starwars |> html_elements("h2")` and make sense of the results.

---

## Main functions in rvest

**`html_text2()` extracts text embedded within tags.** 

Building on the code in the previous slide, here we can use it to get only the text as tring:

```r
title <- films |> 
  html_element("h2") |> 
  html_text2()
title
```

---

## Main functions in rvest

**`html_attr()` gets data out of attributes.** 

It always returns a string so we convert it to an integer: 

```r
episode <- films |> 
  html_element("h2") |> 
  html_attr("data-id") |> 
  readr::parse_integer()
episode
```
---

## Main functions in rvest

**`html_table()` extracts tabular data and converts it directly to a dataframe!**

```r
html <- read_html("https://en.wikipedia.org/w/index.php?title=The_Lego_Movie&oldid=998422565")

html |> 
  html_element(".tracklist") |> 
  html_table()
```

Note: some data cleaning is needed here (and generally to be expected with any scraping project)

---

## Practice all of this in R!

**We scrape this U.S. Presidential statements page: <https://www.presidency.ucsb.edu/documents/special-message-the-congress-relative-space-science-and-exploration>**

What we learn through this example (today and next class):

- Use `rvest` functions to collect name, date, title, and text from that page
- Identify tags using two ways: 
  - examining the webpage directly
  - using helper tools like SelectorGadget
- Learn Scraping best practices
- Scale up our scraper to handle multiple pages

---

## SelectorGadget

Follow these steps to install and use the SelectorGadget...

* Go to <https://selectorgadget.com/> to install and watch a short video on how to use it
* Once installed drag the SelectorGadget link into your web browser's bar
* Navigate to a webpage and open the SelectorGadget bookmark
* Go to <https://rvest.tidyverse.org/articles/selectorgadget.html#use> for step-by-step instructions on how to use it

---

### Web Scraping Resources

* Web scraping 101: <https://rvest.tidyverse.org/articles/rvest.html>

* Rvest tutorial: <https://rvest.tidyverse.org/articles/rvest.html#html-basics>

* Using HTML and CSS for scraping: <https://sscc.wisc.edu/sscc/pubs/webscraping-r/html.html>

* Handy references for your scraping projects:

  * HTML overview: <https://www.w3schools.com/html/html_intro.asp>
  * List of tags: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element> 

